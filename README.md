# Awesome LLM Security Paper

---

![Academic Alpaca](resources/DALLÂ·E%202024-07-30%2015.10.44%20-%20An%20academic-looking%20alpaca%20wearing%20scholarly%20glasses%20and%20a%20graduation%20cap%2C%20with%20an%20intellectual%20and%20serious%20expression.%20The%20background%20should%20be%20a%20lib.webp)

**Languages: [English](README.md) | [ä¸­æ–‡](README_zh.md)**

ğŸ‰ğŸ‰ğŸ‰ **Welcome to the LLM Security Research Repository, your premier destination for the latest and most comprehensive research on LLM security ! ! !**

As the field of artificial intelligence rapidly evolves, so do the security challenges associated with large models. Our repository is at the forefront of this emerging discipline, providing a comprehensive and constantly updated collection of research papers. Our repository stands out for several key reasons:

- ğŸ”¥ **Cutting-Edge Security Research**: Focused on the newest and most innovative security studies targeting large language models, ensuring you stay ahead in this critical area.

- â°ï¸ **Real-Time Updates**: Our repository is updated in real-time, offering the most current and relevant research findings as they become available.

- ğŸ“šï¸ **Comprehensive Coverage**: We aim to cover all aspects of large model security, from theoretical foundations to practical applications, and our collection will continually expand to include every facet of this essential field.

- ğŸ‡¨ğŸ‡³ **High-Quality translation Content**: Access high-quality articles in Chinese, catering to a global audience and promoting cross-language collaboration.

- ğŸŒŸ **Big Model Summary**: Using ChatGPT4 Agent to provide a detailed summary of the paper for a quick understanding of the paper's logic.

Join us in exploring the cutting-edge of AI security and contribute to a safer future for large models. Dive into our extensive and ever-growing collection of research papers today, stay ahead in the rapidly evolving field of AI model security ! ! !

## Prompt injection

| Title | Date | Published | Tag |
|-------|------|-----------|-----|
| [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](paper_list/PoisonedRAG_Knowledge_Poisoning_Attacks_to_Retrieval-Augmented_Generation_of_Large_Language_Models.md) | 2024.2.12 | arXiv | Attack |

## Jailbreaking

| Title | Date | Published | Tag |
|-------|------|-----------|-----|
| [Donâ€™t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models](paper_list/Don't_Listen_To_Me:_Understanding_and_Exploring_Jailbreak_Prompts_of_Large_Language_Models.md) | 2023.3.26 | USENIX Security 2024 | Experiment Attack |

## Data extraction & privacy

| Title | Date | Published | Tag |
|-------|------|-----------|-----|
| [Bag of Tricks for Training Data Extraction from Language Models](paper_list/Bag_of_Tricks_for_Training_Data_Extraction_from_Language_Models.md) | 2023.2.9 | arXiv | Experiment Attack |

## Agent

| Title | Date | Published | Tag |
|-------|------|-----------|-----|
| [The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies](paper_list/The_Emerged_Security_and_Privacy_of_LLM_Agent_A_Survey_with_Case_Studies.md) | 2024.7.28 | arXiv | Survey Attack Defense  |

## Contributing
your contributions are always welcome!

If you have any questions about this paper list, or seek academic research in the field of LLM security, please get in touch at yaojialzc@gmail.com .
